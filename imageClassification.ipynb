{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Packages"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Assets.Loading_Datasets import *"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Datasets\n",
        "\n",
        "Arguments:\n",
        "\n",
        "    train_label:    It declares which fruit this data belongs\n",
        "    train_value:    It has value of every pixle (in this case we extracted the values)\n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            train_set_features:     (1962, 102)\n",
        "            train_set_labels:       (1962, 1)\n",
        "            train_label:            (1962, 4)\n",
        "            train_value1:           (102,200)\n",
        "            train_lable1:           (200, 4)\n",
        "            train_value2:           (102,1762)\n",
        "            train_lable2:           (1762, 4)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "shuffle = np.random.permutation(len(train_set_features))\n",
        "train_set_features = train_set_features[shuffle]\n",
        "train_set_labels = train_set_labels[shuffle]\n",
        "\n",
        "m, n = train_set_features.shape\n",
        "sample_num = 200\n",
        "\n",
        "# sample data\n",
        "train_value1 = train_set_features[0:sample_num].T\n",
        "train_value1 = np.reshape(train_value1, (n, sample_num))\n",
        "assert(train_value1.shape == (n,sample_num))\n",
        "\n",
        "# labels\n",
        "train_label = np.zeros((m,4))\n",
        "for i in range(len(train_set_labels)):\n",
        "    train_label[i][int(train_set_labels[i])] = 1 \n",
        "\n",
        "# sample labels\n",
        "train_label1 = train_label[0:sample_num]\n",
        "train_label1 = np.reshape(train_label1, (sample_num, 4))\n",
        "assert(train_label1.shape == (sample_num, 4))\n",
        "\n",
        "\n",
        "# remain samples data\n",
        "train_value2 = train_set_features[sample_num:m].T\n",
        "assert(train_value2.shape == (n,m-sample_num))\n",
        "\n",
        "# remain samples labels\n",
        "train_label2 = train_label[sample_num:m]\n",
        "train_label2 = np.reshape(train_label2, (m-sample_num, 4))\n",
        "assert(train_label2.shape == (m-sample_num, 4))\n"
      ],
      "execution_count":2,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Initialization\n",
        "layer_dims: python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "parameters: python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "                    Wl: weight matrix \n",
        "                    bl: bias vector\n",
        "\n",
        "\n",
        "procedure:\n",
        "\n",
        "we assign a random number for every weight and a 0 for all biases \n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            Wl: (layer_dims[l], layer_dims[l-1])\n",
        "            bl: (layer_dims[l], 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def initialize_parameters_deep(layer_dims):\n",
        "   \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count":3,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "p = initialize_parameters_deep([102,150,60,4])\n",
        "p['W1']"
      ],
      "execution_count":16,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Network Shape\n",
        "\n",
        "Our network will have 4 layers including output layer.\n",
        "\n",
        "In the input layer, we have 102 items followed by 2 hidden layers with 150 and 60 items.\n",
        "\n",
        "Also, in the output layer, we have for items which are: apple, lemon, mango and raspberry, respectively."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# parameters = initialize_parameters_deep([102,150,60,4])\n",
        "# parameters['W3'].shape\n",
        "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "# print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count":70,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function\n",
        "\n",
        "**Sigmoid:**\n",
        "    \n",
        "    Arguments:\n",
        "    Z:       numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A:       output of sigmoid(z), same shape as Z\n",
        "    cache:   returns Z as well, useful during backpropagation\n",
        "\n",
        "**Relu:**\n",
        "\n",
        "    Arguments:\n",
        "    Z:       Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A:       Post-activation parameter, of the same shape as Z\n",
        "    cache:   storing \"A\" for computing the backward pass efficiently\n",
        "\n",
        "**Dimentions**:\n",
        "                \n",
        "                Z(l): (layer_dims(l), sample_num)\n",
        "                A(l): (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid(Z):\n",
        "    \n",
        "    A = 1\/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count":4,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count":5,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation\n",
        "\n",
        "Implement the **linear part** of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A:      activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:      weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:      bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Z:      the input of the activation function, also called pre-activation parameter.\n",
        "    cache:  storing \"A\", \"W\" and \"b\" for computing the backward pass efficiently.\n",
        "\n",
        "    **Dimensions**:\n",
        "\n",
        "                A:  (layer_dims(l-1), sample_num)\n",
        "                W:  (layer_dims(l), layer_dims(l-1))\n",
        "                b:  (layer_dims(l), 1)\n",
        "                Z:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count":6,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activate the linear part of Forward-Propagation\n",
        "\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION part\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A_prev:     activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:          weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:          bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    A:          the output of the activation function, also called the post-activation value \n",
        "    cache:      storing \"linear_cache\" and \"activation_cache\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
        "        A, activation_cache = sigmoid(Z) \n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) \n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count":7,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation for all layers\n",
        "    \n",
        "**Arguments:**\n",
        "\n",
        "    X:           data, numpy array of shape (input size, number of examples)\n",
        "    parameters:  output of initialize_parameters_deep()\n",
        "    \n",
        "**Returns:**\n",
        "\n",
        "    A:          last post-activation value\n",
        "    caches:      list of caches containing:\n",
        "                    every cache of activating linear part of forward-propagation (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:  (layer_dims(l-1), sample_num)\n",
        "            A:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    assert(A.shape == (4,X.shape[1])) \n",
        "    # --------------------------------------\n",
        "    return A, caches"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "parameters = initialize_parameters_deep([102,150,60,4])\n",
        "A, caches = L_model_forward(train_value1,parameters)\n",
        "# caches = np.array(caches, dtype=object)\n",
        "# caches[0][0][0]"
      ],
      "execution_count":63,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Cost function\n",
        "\n",
        "\n",
        "We can use 2 ways to calculate the cost of network:\n",
        "\n",
        "                                                SSE\n",
        "                                                CE"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**SSE**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_SSE(value, lable):\n",
        "    sse = 0\n",
        "    m = label[0].shape\n",
        "    for j in range(4):\n",
        "        sse += (label[j] - value[j])^2\n",
        "    \n",
        "    return sse"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Cross**-**Entropy**\n",
        "\n",
        "Arguments:\n",
        "\n",
        "        AL:     probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "        Y:      true \"label\" vector\n",
        "\n",
        "Returns:\n",
        "\n",
        "        cost:   cross-entropy cost\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "        cost:  (1)\n",
        "        Y:     (layer_dims(L), sample_num)  -> L is number of layers"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1\/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    cost = cost.sum()\n",
        "    cost = np.squeeze(cost)      # this turns [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count":9,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Back-Propagation\n",
        "\n",
        "We need to calculate Gradient of the cost respect to the parameters. then times it to a learning factor and finally upgrade the parameters.\n",
        "\n",
        "We will use 2 ways:\n",
        "                    one way is that we use some loops to iterate all over the network\n",
        "                    another way is that we use matrix operations "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Using Loops"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def calculate_gradient(value, lable):\n",
        "    for i in range(value.size):\n",
        "        A, caches = L_model_forward(value[i], parameters)\n",
        "        sse = compute_SSE(A.T, lable[i])"
      ],
      "execution_count":36,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Vectorization\n",
        "\n",
        "Using matrix operations"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function Gradient\n",
        "\n",
        "Implement the backward propagation for a single **SIGMOID** unit.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        dA:      post-activation gradient, of any shape\n",
        "        cache:   Z where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        dZ:      Gradient of the cost with respect to Z\n",
        "\n",
        "    Dimensions:\n",
        "\n",
        "                dA:     (layer_dims(l), sample_num)\n",
        "                Z:      (layer_dims(l-1), sample_num)\n",
        "                dZ:     (layer_dims(l-1), sample_num)\n",
        "\n",
        "                here A in layer_dims(l) and Z in layer_dims(l-1) have equal dimentions."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1\/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Gradient of parameters for a singl layer\n",
        "\n",
        "Here **cache** is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "   \n",
        "\n",
        "Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:      (layer_dims(l-1), sample_num)\n",
        "            W:           (layer_dims(l), layer_dims(l-1))\n",
        "            b:           (layer_dims(l), 1)\n",
        "            dz:          (layer_dims(l), sample_num)\n",
        "            dA_prev:     (layer_dims(l-1), sample_num)\n",
        "            dW:          (layer_dims(l), layer_dims(l-1))\n",
        "            db:          (layer_dims(l), 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1\/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1\/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":18,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":12,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Obtain Gradient\n",
        "\n",
        "Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_backward(AL, Y, caches):\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    current_cache = caches[L-1] # Last Layer\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "#    فک کنم اینجا باید مستقیم نود لایه اخر رو بدیم یعنی نیازی به سیگموید نداره\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"sigmoid\")\n",
        "\n",
        "# ----------------------------------------\n",
        "    return grads"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Update Parameters\n",
        "\n",
        "Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ..."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) \/\/ 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "execution_count":20,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Implement a L-layer neural network\n",
        "\n",
        " Arguments:\n",
        "\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    parameters -- parameters learnt by the model. They can then be used to predict."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_layer_model(data, lable, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation:\n",
        "        AL, caches = L_model_forward(data, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, lable)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, lable, caches)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 10 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count":23,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "layers_dims = [102, 150, 60, 4]\n",
        "parameters = L_layer_model(train_value1, train_label1.T, layers_dims, 0.0075, 100, True)\n",
        "\n",
        "# parameters = initialize_parameters_deep(layers_dims)\n",
        "# Al, caches = L_model_forward(train_value1, parameters)\n",
        "# cost = compute_cost(Al, train_lable1[0])\n",
        "# cost"
      ],
      "execution_count":24,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Cost after iteration 0: 11.175703\n",
            "Cost after iteration 10: 10.228268\n",
            "Cost after iteration 20: 9.709856\n",
            "Cost after iteration 30: 9.421922\n",
            "Cost after iteration 40: 9.260106\n",
            "Cost after iteration 50: 9.168632\n",
            "Cost after iteration 60: 9.117029\n",
            "Cost after iteration 70: 9.088294\n",
            "Cost after iteration 80: 9.072763\n",
            "Cost after iteration 90: 9.064861\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "image\/png":[
              "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\n"
            ]
          },
          "metadata":{
            "image\/png":{
              
            }
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# pred_train = predict(train_value1, train_lable1, parameters)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}