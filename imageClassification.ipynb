{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Packages"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Assets.Loading_Datasets import *"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Datasets\n",
        "\n",
        "Arguments:\n",
        "\n",
        "    train_label:    It declares which fruit this data belongs\n",
        "    train_value:    It has value of every pixle (in this case we extracted the values)\n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            train_set_features:     (1962, 102)\n",
        "            train_set_labels:       (1962, 1)\n",
        "            train_label:            (1962, 4)\n",
        "            train_value1:           (102,200)\n",
        "            train_lable1:           (200, 4)\n",
        "            train_value2:           (102,1762)\n",
        "            train_lable2:           (1762, 4)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "shuffle = np.random.permutation(len(train_set_features))\n",
        "train_set_feature = train_set_features[shuffle]\n",
        "train_set_label = train_set_labels[shuffle]\n",
        "\n",
        "m, n = train_set_feature.shape\n",
        "sample_num = 200\n",
        "\n",
        "# labels\n",
        "train_label = np.zeros((m,4))\n",
        "for i in range(len(train_set_label)):\n",
        "    train_label[i][int(train_set_label[i])] = 1 \n",
        "\n",
        "# sample data\n",
        "train_value1 = train_set_feature[0:sample_num].T\n",
        "train_value1 = np.reshape(train_value1, (n, sample_num))\n",
        "assert(train_value1.shape == (n,sample_num))\n",
        "\n",
        "# sample labels\n",
        "train_label1 = train_label[0:sample_num]\n",
        "train_label1 = np.reshape(train_label1, (sample_num, 4))\n",
        "assert(train_label1.shape == (sample_num, 4))\n",
        "\n",
        "\n",
        "# remain samples data\n",
        "train_value2 = train_set_feature[sample_num:m].T\n",
        "assert(train_value2.shape == (n,m-sample_num))\n",
        "\n",
        "# remain samples labels\n",
        "train_label2 = train_label[sample_num:m]\n",
        "train_label2 = np.reshape(train_label2, (m-sample_num, 4))\n",
        "assert(train_label2.shape == (m-sample_num, 4))\n"
      ],
      "execution_count":176,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Initialization\n",
        "layer_dims: python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "parameters: python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "                    Wl: weight matrix \n",
        "                    bl: bias vector\n",
        "\n",
        "\n",
        "procedure:\n",
        "\n",
        "we assign a random number for every weight and a 0 for all biases \n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            Wl: (layer_dims[l], layer_dims[l-1])\n",
        "            bl: (layer_dims[l], 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def initialize_parameters_deep(layer_dims):\n",
        "   \n",
        "    # np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count":3,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# p = initialize_parameters_deep([5, 4, 3])\n",
        "# print(\"W1 = \" + str(p[\"W1\"]))\n",
        "# print(\"b1 = \" + str(p[\"b1\"]))\n",
        "# print(\"W2 = \" + str(p[\"W2\"]))\n",
        "# print(\"b2 = \" + str(p[\"b2\"]))"
      ],
      "execution_count":45,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Network Shape\n",
        "\n",
        "Our network will have 4 layers including output layer.\n",
        "\n",
        "In the input layer, we have 102 items followed by 2 hidden layers with 150 and 60 items.\n",
        "\n",
        "Also, in the output layer, we have for items which are: apple, lemon, mango and raspberry, respectively."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function\n",
        "\n",
        "**Sigmoid:**\n",
        "    \n",
        "    Arguments:\n",
        "    Z:       numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A:       output of sigmoid(z), same shape as Z\n",
        "    cache:   returns Z as well, useful during backpropagation\n",
        "\n",
        "**Relu:**\n",
        "\n",
        "    Arguments:\n",
        "    Z:       Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A:       Post-activation parameter, of the same shape as Z\n",
        "    cache:   storing \"A\" for computing the backward pass efficiently\n",
        "\n",
        "**Dimentions**:\n",
        "                \n",
        "                Z(l): (layer_dims(l), sample_num)\n",
        "                A(l): (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid(Z):\n",
        "    \n",
        "    A = 1\/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count":181,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count":5,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation\n",
        "\n",
        "Implement the **linear part** of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A:      activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:      weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:      bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Z:      the input of the activation function, also called pre-activation parameter.\n",
        "    cache:  storing \"A\", \"W\" and \"b\" for computing the backward pass efficiently.\n",
        "\n",
        "    **Dimensions**:\n",
        "\n",
        "                A:  (layer_dims(l-1), sample_num)\n",
        "                W:  (layer_dims(l), layer_dims(l-1))\n",
        "                b:  (layer_dims(l), 1)\n",
        "                Z:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count":6,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# A = np.random.randn(3,2)\n",
        "# W = np.random.randn(1,3)\n",
        "# b = np.random.randn(1,1)\n",
        "# Z, linear_cache = linear_forward(A, W, b)\n",
        "# print(\"Z = \" + str(Z))"
      ],
      "execution_count":44,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activate the linear part of Forward-Propagation\n",
        "\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION part\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A_prev:     activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:          weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:          bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    A:          the output of the activation function, also called the post-activation value \n",
        "    cache:      storing \"linear_cache\" and \"activation_cache\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
        "        A, activation_cache = sigmoid(Z) \n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) \n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count":47,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# A_prev = np.random.randn(3,2)\n",
        "# W = np.random.randn(1,3)\n",
        "# b = np.random.randn(1,1)\n",
        "\n",
        "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
        "# print(\"With sigmoid: A = \" + str(A))\n",
        "\n",
        "# A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
        "# print(\"With ReLU: A = \" + str(A))"
      ],
      "execution_count":51,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation for all layers\n",
        "    \n",
        "**Arguments:**\n",
        "\n",
        "    X:           data, numpy array of shape (input size, number of examples)\n",
        "    parameters:  output of initialize_parameters_deep()\n",
        "    \n",
        "**Returns:**\n",
        "\n",
        "    A:          last post-activation value\n",
        "    caches:      list of caches containing:\n",
        "                    every cache of activating linear part of forward-propagation (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:  (layer_dims(l-1), sample_num)\n",
        "            A:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    Al, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(Al.shape == (4,X.shape[1])) \n",
        "    # --------------------------------------\n",
        "    return Al, caches"
      ],
      "execution_count":207,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# X = np.random.randn(5,4)\n",
        "# W1 = np.random.randn(4,5)\n",
        "# b1 = np.random.randn(4,1)\n",
        "# W2 = np.random.randn(3,4)\n",
        "# b2 = np.random.randn(3,1)\n",
        "# W3 = np.random.randn(1,3)\n",
        "# b3 = np.random.randn(1,1)\n",
        "\n",
        "# p = {\"W1\": W1,\n",
        "#      \"b1\": b1,\n",
        "#      \"W2\": W2,\n",
        "#      \"b2\": b2,\n",
        "#      \"W3\": W3,\n",
        "#      \"b3\": b3}\n",
        "# AL, caches = L_model_forward(X, p)\n",
        "# print(\"AL = \" + str(AL))\n",
        "# print(\"Length of caches list = \" + str(len(caches)))"
      ],
      "execution_count":56,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Cost function\n",
        "\n",
        "\n",
        "We can use 2 ways to calculate the cost of network:\n",
        "\n",
        "                                                SSE\n",
        "                                                CE"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**SSE**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_SSE(a, g):\n",
        "   \n",
        "    sse = np.square(a-g)\n",
        "    sse = np.sum(sse)\/len(g)\n",
        "    return(sse)"
      ],
      "execution_count":9,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# a = np.array([[1, 1], [1,2]])\n",
        "# g = np.array([[1, 0], [1,2]])\n",
        "\n",
        "# compute_SSE(a,g)"
      ],
      "execution_count":47,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Cross**-**Entropy**\n",
        "\n",
        "Arguments:\n",
        "\n",
        "        AL:     probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "        Y:      true \"label\" vector\n",
        "\n",
        "Returns:\n",
        "\n",
        "        cost:   cross-entropy cost\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "        cost:  (1)\n",
        "        Y:     (layer_dims(L), sample_num)  -> L is number of layers"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1\/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    cost = cost.sum()\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count":61,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "\n",
        "# Y = np.asarray([[1, 1, 0]])\n",
        "# AL = np.array([[.8,.9,0.4]])\n",
        "# print(\"cost = \" + str(compute_cost(AL, Y)))"
      ],
      "execution_count":64,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Backward-Propagation\n",
        "\n",
        "We need to calculate Gradient of the cost respect to the parameters. then times it to a learning factor and finally upgrade the parameters.\n",
        "\n",
        "We will use 2 ways:\n",
        "                    one way is that we use some loops to iterate all over the network\n",
        "                    another way is that we use matrix operations "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Using Loops"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def calculate_gradient(value, lable):\n",
        "    for i in range(value.size):\n",
        "        A, caches = L_model_forward(value[i], parameters)\n",
        "        sse = compute_SSE(A.T, lable[i])"
      ],
      "execution_count":14,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Vectorization\n",
        "\n",
        "Using matrix operations"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function Gradient\n",
        "\n",
        "Implement the backward propagation for a single **SIGMOID** unit.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        dA:      post-activation gradient, of any shape\n",
        "        cache:   Z where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        dZ:      Gradient of the cost with respect to Z\n",
        "\n",
        "    Dimensions:\n",
        "\n",
        "                dA:     (layer_dims(l), sample_num)\n",
        "                Z:      (layer_dims(l-1), sample_num)\n",
        "                dZ:     (layer_dims(l-1), sample_num)\n",
        "\n",
        "                here A in layer_dims(l) and Z in layer_dims(l-1) have equal dimentions."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid_backward(dA, cache): # ---------------------------remove da---------------------------------\n",
        "    \n",
        "    Z = cache\n",
        "    s = 1\/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":182,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":194,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Gradient of parameters for a singl layer\n",
        "\n",
        "Here **cache** is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "   \n",
        "\n",
        "Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:      (layer_dims(l-1), sample_num)\n",
        "            W:           (layer_dims(l), layer_dims(l-1))\n",
        "            b:           (layer_dims(l), 1)\n",
        "            dz:          (layer_dims(l), sample_num)\n",
        "            dA_prev:     (layer_dims(l-1), sample_num)\n",
        "            dW:          (layer_dims(l), layer_dims(l-1))\n",
        "            db:          (layer_dims(l), 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1\/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1\/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":68,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# dZ = np.random.randn(3,4)\n",
        "# A = np.random.randn(5,4)\n",
        "# W = np.random.randn(3,5)\n",
        "# b = np.random.randn(3,1)\n",
        "# linear_cache = (A, W, b)\n",
        "# dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "# print (\"dA_prev = \"+ str(dA_prev))\n",
        "# print (\"dW = \" + str(dW))\n",
        "# print (\"db = \" + str(db))"
      ],
      "execution_count":71,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":72,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# dAL = np.random.randn(1,2)\n",
        "# A = np.random.randn(3,2)\n",
        "# W = np.random.randn(1,3)\n",
        "# b = np.random.randn(1,1)\n",
        "# Z = np.random.randn(1,2)\n",
        "# linear_cache = (A, W, b)\n",
        "# activation_cache = Z\n",
        "# linear_activation_cache = (linear_cache, activation_cache)\n",
        "# dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
        "# print (\"sigmoid:\")\n",
        "# print (\"dA_prev = \"+ str(dA_prev))\n",
        "# print (\"dW = \" + str(dW))\n",
        "# print (\"db = \" + str(db) + \"\\n\")"
      ],
      "execution_count":83,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Obtain Gradient\n",
        "\n",
        "    Arguments:\n",
        "    AL: probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y: true \"label\" vector \n",
        "    caches: list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads: A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_backward(AL, Y, caches): #----------------------------------------------------------------------------\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    # dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #------------------------------------------------------------\n",
        "    cost_drive = 2 * (AL - Y)\n",
        "    current_cache = caches[L-1] # Last Layer\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(cost_drive, current_cache, \"sigmoid\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, \"sigmoid\")\n",
        "\n",
        "    return grads"
      ],
      "execution_count":215,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# AL = np.random.randn(1, 2)\n",
        "# Y = np.array([[1, 0]])\n",
        "\n",
        "# A1 = np.random.randn(4,2)\n",
        "# W1 = np.random.randn(3,4)\n",
        "# b1 = np.random.randn(3,1)\n",
        "# Z1 = np.random.randn(3,2)\n",
        "# linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
        "\n",
        "# A2 = np.random.randn(3,2)\n",
        "# W2 = np.random.randn(1,3)\n",
        "# b2 = np.random.randn(1,1)\n",
        "# Z2 = np.random.randn(1,2)\n",
        "# linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
        "\n",
        "# caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
        "# grads = L_model_backward(AL, Y, caches)\n",
        "# print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
        "# print (\"db1 = \"+ str(grads[\"db1\"]))\n",
        "# print (\"dA1 = \"+ str(grads[\"dA1\"]))  "
      ],
      "execution_count":89,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Update Parameters\n",
        "\n",
        "Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ..."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    param = {}\n",
        "    L = len(parameters) \/\/ 2 # number of layers in the neural network\n",
        "    \n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "execution_count":15,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# W1 = np.random.randn(3,4)\n",
        "# b1 = np.random.randn(3,1)\n",
        "# W2 = np.random.randn(1,3)\n",
        "# b2 = np.random.randn(1,1)\n",
        "# p = {\"W1\": W1,\n",
        "#                 \"b1\": b1,\n",
        "#                 \"W2\": W2,\n",
        "#                 \"b2\": b2}\n",
        "# np.random.seed(3)\n",
        "# dW1 = np.random.randn(3,4)\n",
        "# db1 = np.random.randn(3,1)\n",
        "# dW2 = np.random.randn(1,3)\n",
        "# db2 = np.random.randn(1,1)\n",
        "# grads = {\"dW1\": dW1,\n",
        "#             \"db1\": db1,\n",
        "#             \"dW2\": dW2,\n",
        "#             \"db2\": db2}\n",
        "\n",
        "# print (\"W1 = \"+ str(p[\"W1\"]))\n",
        "\n",
        "# p = update_parameters(p, grads, 0.1)\n",
        "\n",
        "# print (\"W1 = \"+ str(p[\"W1\"]))"
      ],
      "execution_count":92,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Prediction"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_predictions(A, parameters):\n",
        "\n",
        "    m = A.shape[1]\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(A, parameters)\n",
        "    probas = np.reshape(probas, [sample_num,4])\n",
        "    # convert probas to 0\/1 predictions\n",
        "    p = np.argmax(probas,1)\n",
        "    # for i in range(train_label.shape[0]):\n",
        "    #     print(\"label: \", train_label[i])\n",
        "    # print(\"p: \", p)\n",
        "    \n",
        "    print(\"Accuracy: \"  + str(np.sum((p == train_set_labels[:sample_num])\/m)))\n",
        "        \n",
        "    return p\n",
        "\n",
        "\n",
        "    # return np.argmax(A, 0)"
      ],
      "execution_count":168,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Implement a L-layer neural network\n",
        "\n",
        " Arguments:\n",
        "\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    parameters -- parameters learnt by the model. They can then be used to predict."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_layer_model(data, label, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "    \n",
        "        \n",
        "        # shuffle = np.random.permutation(data.shape[1])\n",
        "        # data = data.T[shuffle]\n",
        "        # label = label[shuffle]\n",
        "\n",
        "        # Forward propagation:\n",
        "        AL, caches = L_model_forward(data, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        # cost = compute_SSE(AL, label)\n",
        "        cost = compute_cost(AL, label)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, label, caches)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            predictions = get_predictions(data, parameters)\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "   "
      ],
      "execution_count":214,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "layers_dims = [102, 150, 60, 4] \n",
        "L_layer_model(train_value1, train_label1.T, layers_dims, 0.5, 800, True)"
      ],
      "execution_count":216,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Cost after iteration 0: 10.940632\n",
            "Accuracy: 0.39\n",
            "Cost after iteration 100: 9.074534\n",
            "Accuracy: 0.39\n",
            "Cost after iteration 200: 9.074534\n",
            "Accuracy: 0.4\n",
            "Cost after iteration 300: 9.074534\n",
            "Accuracy: 0.405\n",
            "Cost after iteration 400: 9.074534\n",
            "Accuracy: 0.395\n",
            "Cost after iteration 500: 9.074534\n",
            "Accuracy: 0.41000000000000003\n",
            "Cost after iteration 600: 9.074534\n",
            "Accuracy: 0.41000000000000003\n",
            "Cost after iteration 700: 9.074534\n",
            "Accuracy: 0.42000000000000004\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "image\/png":[
              "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\n"
            ]
          },
          "metadata":{
            "image\/png":{
              
            }
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}