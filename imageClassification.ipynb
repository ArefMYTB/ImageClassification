{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Packages"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Assets.Loading_Datasets import *\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Datasets\n",
        "\n",
        "Arguments:\n",
        "\n",
        "    train_lable:    It declares which fruit this data belongs\n",
        "    train_value:    It has value of every pixle"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "train_data = np.array(train_set, dtype=object)\n",
        "\n",
        "#train_data\n",
        "\n",
        "m, n = train_data.shape\n",
        "\n",
        "np.random.shuffle(train_data)\n",
        "\n",
        "data_train1 = train_data[0:200].T\n",
        "train_value1 = data_train1[0]\n",
        "train_lable1 = data_train1[1:n]\n",
        "\n",
        "data_train2 = train_data[200:m].T\n",
        "train_value2 = data_train2[0]\n",
        "train_lable2 = data_train2[1:n]\n",
        "\n",
        "train_value1"
      ],
      "execution_count":2,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Initialization\n",
        "layer_dims: python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "parameters: python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "                    Wl: weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl: bias vector of shape (layer_dims[l], 1)\n",
        "\n",
        "\n",
        "procedure:\n",
        "\n",
        "we assign a random number for every weight and a 0 for all biases "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def initialize_parameters_deep(layer_dims):\n",
        "   \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count":3,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Network Shape\n",
        "\n",
        "Our network will have 4 layers including output layer.\n",
        "\n",
        "In the input layer, we have 102 items followed by 2 hidden layers with 150 and 60 items.\n",
        "\n",
        "Also, in the output layer, we have for items which are: apple, lemon, mango and raspberry, respectively."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "parameters = initialize_parameters_deep([102,150,60,4])\n",
        "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "# print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count":4,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function\n",
        "\n",
        "**Sigmoid:**\n",
        "    \n",
        "    Arguments:\n",
        "    Z:       numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A:       output of sigmoid(z), same shape as Z\n",
        "    cache:   returns Z as well, useful during backpropagation\n",
        "\n",
        "**Relu:**\n",
        "\n",
        "    Arguments:\n",
        "    Z:       Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A:       Post-activation parameter, of the same shape as Z\n",
        "    cache:   storing \"A\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid(Z):\n",
        "    \n",
        "    A = 1\/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count":5,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count":6,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation\n",
        "\n",
        "Implement the **linear part** of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A:      activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:      weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:      bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Z:      the input of the activation function, also called pre-activation parameter.\n",
        "    cache:  storing \"A\", \"W\" and \"b\" for computing the backward pass efficiently."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    # assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count":7,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activate the linear part of Forward-Propagation\n",
        "\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION part\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A_prev:     activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:          weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:          bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    A:          the output of the activation function, also called the post-activation value \n",
        "    cache:      storing \"linear_cache\" and \"activation_cache\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
        "        A, activation_cache = sigmoid(Z) \n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) \n",
        "    \n",
        "    # assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count":11,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation for all layers\n",
        "    \n",
        "**Arguments:**\n",
        "\n",
        "    X:           data, numpy array of shape (input size, number of examples)\n",
        "    parameters:  output of initialize_parameters_deep()\n",
        "    \n",
        "**Returns:**\n",
        "\n",
        "    A:          last post-activation value\n",
        "    caches:      list of caches containing:\n",
        "                    every cache of activating linear part of forward-propagation (there are L-1 of them, indexed from 0 to L-1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # assert(A.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return A, caches"
      ],
      "execution_count":12,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Cost function\n",
        "\n",
        "\n",
        "We can use 2 ways to calculate the cost of network:\n",
        "\n",
        "                                                SSE\n",
        "                                                CE"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**SSE**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_SSE(value, lable):\n",
        "    sse = 0\n",
        "    m = label[0].shape\n",
        "    for j in range(4):\n",
        "        sse += (label[j] - value[j])^2\n",
        "    \n",
        "    return sse"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Cross**-**Entropy**\n",
        "\n",
        "Arguments:\n",
        "\n",
        "        AL:     probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "        Y:      true \"label\" vector\n",
        "\n",
        "Returns:\n",
        "\n",
        "        cost:   cross-entropy cost"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_cost(AL, Y):\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1\/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    \n",
        "    # cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    # assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count":14,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Back-Propagation\n",
        "\n",
        "We need to calculate Gradient of the cost respect to the parameters. then times it to a learning factor and finally upgrade the parameters.\n",
        "\n",
        "We will use 2 ways:\n",
        "                    one way is that we use some loops to iterate all over the network\n",
        "                    another way is that we use matrix operations "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Using Loops"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def calculate_gradient(value, lable):\n",
        "    for i in range(value.size):\n",
        "        A, caches = L_model_forward(value[i], parameters)\n",
        "        sse = compute_SSE(A.T, lable[i])"
      ],
      "execution_count":36,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Vectorization\n",
        "\n",
        "Using matrix operations"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function Gradient\n",
        "\n",
        "Implement the backward propagation for a single **SIGMOID** unit.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        dA:      post-activation gradient, of any shape\n",
        "        cache:   Z where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        dZ:      Gradient of the cost with respect to Z"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1\/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":15,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Gradient of parameters for a singl layer\n",
        "\n",
        "Here **cache** is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "   \n",
        "\n",
        "Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1\/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1\/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":16,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":17,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Obtain Gradient\n",
        "\n",
        "Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_backward(AL, Y, caches):\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1] # Last Layer\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count":18,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Update Parameters\n",
        "\n",
        "Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ..."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) \/\/ 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "execution_count":12,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# parameters, grads = update_parameters_test_case()\n",
        "# parameters = update_parameters(parameters, grads, 0.1)\n",
        "\n",
        "# print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
        "# print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
        "# print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
        "# print (\"b2 = \"+ str(parameters[\"b2\"]))"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}