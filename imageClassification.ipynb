{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Packages"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Assets.Loading_Datasets import *"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Datasets\n",
        "\n",
        "Arguments:\n",
        "\n",
        "    train_label:    It declares which fruit this data belongs\n",
        "    train_value:    It has value of every pixle (in this case we extracted the values)\n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            train_set_features:     (1962, 102)\n",
        "            train_set_labels:       (1962, 1)\n",
        "            train_label:            (1962, 4)\n",
        "            train_value1:           (102,200)\n",
        "            train_lable1:           (200, 4)\n",
        "            train_value2:           (102,1762)\n",
        "            train_lable2:           (1762, 4)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "shuffle = np.random.permutation(len(train_set_features))\n",
        "train_set_feature = train_set_features[shuffle]\n",
        "train_set_label = train_set_labels[shuffle]\n",
        "\n",
        "m, n = train_set_feature.shape\n",
        "sample_num = 200\n",
        "\n",
        "# labels\n",
        "train_label = np.zeros((m,4))\n",
        "for i in range(len(train_set_label)):\n",
        "    train_label[i][int(train_set_label[i])] = 1 \n",
        "\n",
        "# sample data\n",
        "train_value1 = train_set_feature[0:sample_num].T\n",
        "train_value1 = np.reshape(train_value1, (n, sample_num))\n",
        "assert(train_value1.shape == (n,sample_num))\n",
        "\n",
        "# sample labels\n",
        "train_label1 = train_label[0:sample_num]\n",
        "train_label1 = np.reshape(train_label1, (sample_num, 4))\n",
        "assert(train_label1.shape == (sample_num, 4))\n",
        "\n",
        "\n",
        "# remain samples data\n",
        "train_value2 = train_set_feature[sample_num:m].T\n",
        "assert(train_value2.shape == (n,m-sample_num))\n",
        "\n",
        "# remain samples labels\n",
        "train_label2 = train_label[sample_num:m]\n",
        "train_label2 = np.reshape(train_label2, (m-sample_num, 4))\n",
        "assert(train_label2.shape == (m-sample_num, 4))\n"
      ],
      "execution_count":38,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Initialization\n",
        "layer_dims: python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "parameters: python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "                    Wl: weight matrix \n",
        "                    bl: bias vector\n",
        "\n",
        "\n",
        "procedure:\n",
        "\n",
        "we assign a random number for every weight and a 0 for all biases \n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            Wl: (layer_dims[l], layer_dims[l-1])\n",
        "            bl: (layer_dims[l], 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def initialize_parameters_deep(layer_dims):\n",
        "   \n",
        "    # np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count":7,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "p = initialize_parameters_deep([102,150,60,4])\n",
        "p['W1']"
      ],
      "execution_count":24,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Network Shape\n",
        "\n",
        "Our network will have 4 layers including output layer.\n",
        "\n",
        "In the input layer, we have 102 items followed by 2 hidden layers with 150 and 60 items.\n",
        "\n",
        "Also, in the output layer, we have for items which are: apple, lemon, mango and raspberry, respectively."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# parameters = initialize_parameters_deep([102,150,60,4])\n",
        "# parameters['W3'].shape\n",
        "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "# print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count":70,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function\n",
        "\n",
        "**Sigmoid:**\n",
        "    \n",
        "    Arguments:\n",
        "    Z:       numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A:       output of sigmoid(z), same shape as Z\n",
        "    cache:   returns Z as well, useful during backpropagation\n",
        "\n",
        "**Relu:**\n",
        "\n",
        "    Arguments:\n",
        "    Z:       Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A:       Post-activation parameter, of the same shape as Z\n",
        "    cache:   storing \"A\" for computing the backward pass efficiently\n",
        "\n",
        "**Dimentions**:\n",
        "                \n",
        "                Z(l): (layer_dims(l), sample_num)\n",
        "                A(l): (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid(Z):\n",
        "    \n",
        "    A = 1\/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count":9,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation\n",
        "\n",
        "Implement the **linear part** of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A:      activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:      weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:      bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Z:      the input of the activation function, also called pre-activation parameter.\n",
        "    cache:  storing \"A\", \"W\" and \"b\" for computing the backward pass efficiently.\n",
        "\n",
        "    **Dimensions**:\n",
        "\n",
        "                A:  (layer_dims(l-1), sample_num)\n",
        "                W:  (layer_dims(l), layer_dims(l-1))\n",
        "                b:  (layer_dims(l), 1)\n",
        "                Z:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activate the linear part of Forward-Propagation\n",
        "\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION part\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A_prev:     activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:          weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:          bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    A:          the output of the activation function, also called the post-activation value \n",
        "    cache:      storing \"linear_cache\" and \"activation_cache\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
        "        A, activation_cache = sigmoid(Z) \n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) \n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count":11,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation for all layers\n",
        "    \n",
        "**Arguments:**\n",
        "\n",
        "    X:           data, numpy array of shape (input size, number of examples)\n",
        "    parameters:  output of initialize_parameters_deep()\n",
        "    \n",
        "**Returns:**\n",
        "\n",
        "    A:          last post-activation value\n",
        "    caches:      list of caches containing:\n",
        "                    every cache of activating linear part of forward-propagation (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:  (layer_dims(l-1), sample_num)\n",
        "            A:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    assert(A.shape == (4,X.shape[1])) \n",
        "    # --------------------------------------\n",
        "    return A, caches"
      ],
      "execution_count":12,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "parameters = initialize_parameters_deep([102,150,60,4])\n",
        "A, caches = L_model_forward(train_value1,parameters)\n",
        "# caches = np.array(caches, dtype=object)\n",
        "# caches[0][0][0]"
      ],
      "execution_count":63,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Cost function\n",
        "\n",
        "\n",
        "We can use 2 ways to calculate the cost of network:\n",
        "\n",
        "                                                SSE\n",
        "                                                CE"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**SSE**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_SSE(value, lable):\n",
        "   \n",
        "    sse =(np.square(value-label))\n",
        "    sse = np.sum(sse)\/len(label)\n",
        "    return(sse)"
      ],
      "execution_count":30,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Cross**-**Entropy**\n",
        "\n",
        "Arguments:\n",
        "\n",
        "        AL:     probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "        Y:      true \"label\" vector\n",
        "\n",
        "Returns:\n",
        "\n",
        "        cost:   cross-entropy cost\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "        cost:  (1)\n",
        "        Y:     (layer_dims(L), sample_num)  -> L is number of layers"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1\/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    cost = cost.sum()\n",
        "    cost = np.squeeze(cost)      # this turns [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Back-Propagation\n",
        "\n",
        "We need to calculate Gradient of the cost respect to the parameters. then times it to a learning factor and finally upgrade the parameters.\n",
        "\n",
        "We will use 2 ways:\n",
        "                    one way is that we use some loops to iterate all over the network\n",
        "                    another way is that we use matrix operations "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Using Loops"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def calculate_gradient(value, lable):\n",
        "    for i in range(value.size):\n",
        "        A, caches = L_model_forward(value[i], parameters)\n",
        "        sse = compute_SSE(A.T, lable[i])"
      ],
      "execution_count":36,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Vectorization\n",
        "\n",
        "Using matrix operations"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function Gradient\n",
        "\n",
        "Implement the backward propagation for a single **SIGMOID** unit.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        dA:      post-activation gradient, of any shape\n",
        "        cache:   Z where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        dZ:      Gradient of the cost with respect to Z\n",
        "\n",
        "    Dimensions:\n",
        "\n",
        "                dA:     (layer_dims(l), sample_num)\n",
        "                Z:      (layer_dims(l-1), sample_num)\n",
        "                dZ:     (layer_dims(l-1), sample_num)\n",
        "\n",
        "                here A in layer_dims(l) and Z in layer_dims(l-1) have equal dimentions."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1\/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":14,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Gradient of parameters for a singl layer\n",
        "\n",
        "Here **cache** is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "   \n",
        "\n",
        "Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:      (layer_dims(l-1), sample_num)\n",
        "            W:           (layer_dims(l), layer_dims(l-1))\n",
        "            b:           (layer_dims(l), 1)\n",
        "            dz:          (layer_dims(l), sample_num)\n",
        "            dA_prev:     (layer_dims(l-1), sample_num)\n",
        "            dW:          (layer_dims(l), layer_dims(l-1))\n",
        "            db:          (layer_dims(l), 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1\/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1\/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":15,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":21,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Obtain Gradient\n",
        "\n",
        "    Arguments:\n",
        "    AL: probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y: true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches: list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads: A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_backward(AL, Y, caches):\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) #------------------------------------------------------------\n",
        "\n",
        "    current_cache = caches[L-1] # Last Layer\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"sigmoid\")\n",
        "\n",
        "# ----------------------------------------\n",
        "    return grads"
      ],
      "execution_count":16,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Update Parameters\n",
        "\n",
        "Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ..."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    param = {}\n",
        "    L = len(parameters) \/\/ 2 # number of layers in the neural network\n",
        "    \n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "execution_count":17,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Prediction"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_predictions(A, parameters):\n",
        "\n",
        "    m = A.shape[1]\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(A, parameters)\n",
        "    probas = np.reshape(probas, [sample_num,4])\n",
        "    # convert probas to 0\/1 predictions\n",
        "    p = np.argmax(probas,1)\n",
        "    # print(\"label: \", train_label)\n",
        "    # print(\"p: \", p)\n",
        "    \n",
        "    print(\"Accuracy: \"  + str(np.sum((p == train_set_labels[:sample_num])\/m)))\n",
        "        \n",
        "    return p\n",
        "\n",
        "\n",
        "    # return np.argmax(A, 0)"
      ],
      "execution_count":42,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Implement a L-layer neural network\n",
        "\n",
        " Arguments:\n",
        "\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    parameters -- parameters learnt by the model. They can then be used to predict."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_layer_model(data, label, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "    \n",
        "        \n",
        "        # shuffle = np.random.permutation(data.shape[1])\n",
        "        # data = data.T[shuffle]\n",
        "        # label = label[shuffle]\n",
        "\n",
        "        # Forward propagation:\n",
        "        AL, caches = L_model_forward(data, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, label)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, label, caches)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            predictions = get_predictions(data, parameters)\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count":40,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "layers_dims = [102, 150, 60, 4]\n",
        "parameters = L_layer_model(train_value1, train_label1.T, layers_dims, 0.01, 50, True)"
      ],
      "execution_count":46,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Cost after iteration 0: 10.940632\n",
            "Accuracy: 0.275\n",
            "Cost after iteration 10: 9.905228\n",
            "Accuracy: 0.295\n",
            "Cost after iteration 20: 9.438438\n",
            "Accuracy: 0.265\n",
            "Cost after iteration 30: 9.222277\n",
            "Accuracy: 0.265\n",
            "Cost after iteration 40: 9.120401\n",
            "Accuracy: 0.26\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "image\/png":[
              "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\n"
            ]
          },
          "metadata":{
            "image\/png":{
              
            }
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# pred_train = predict(train_value1, train_lable1, parameters)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}