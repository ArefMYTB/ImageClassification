{
  "cells":[
    {
      "cell_type":"markdown",
      "source":[
        "# Packages"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from Assets.Loading_Datasets import *"
      ],
      "execution_count":1,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Datasets\n",
        "\n",
        "Arguments:\n",
        "\n",
        "    train_label:    It declares which fruit this data belongs\n",
        "    train_value:    It has value of every pixle (in this case we extracted the values)\n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            train_set_features:     (1962, 102)\n",
        "            train_set_labels:       (1962, 1)\n",
        "            train_label:            (1962, 4)\n",
        "            train_value1:           (102,200)\n",
        "            train_lable1:           (200, 4)\n",
        "            train_value2:           (102,1762)\n",
        "            train_lable2:           (1762, 4)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "shuffle = np.random.permutation(len(train_set_features))\n",
        "train_set_features = train_set_features[shuffle]\n",
        "train_set_labels = train_set_labels[shuffle]\n",
        "\n",
        "m, n = train_set_features.shape\n",
        "sample_num = 200\n",
        "\n",
        "# labels\n",
        "train_label = np.zeros((m,4))\n",
        "for i in range(len(train_set_labels)):\n",
        "    train_label[i][int(train_set_labels[i])] = 1 \n",
        "\n",
        "# sample data\n",
        "train_value1 = train_set_features[0:sample_num].T\n",
        "train_value1 = np.reshape(train_value1, (n, sample_num))\n",
        "assert(train_value1.shape == (n,sample_num))\n",
        "\n",
        "# sample labels\n",
        "train_label1 = train_label[0:sample_num]\n",
        "train_label1 = np.reshape(train_label1, (sample_num, 4))\n",
        "assert(train_label1.shape == (sample_num, 4))\n",
        "\n",
        "\n",
        "# remain samples data\n",
        "train_value2 = train_set_features[sample_num:m].T\n",
        "assert(train_value2.shape == (n,m-sample_num))\n",
        "\n",
        "# remain samples labels\n",
        "train_label2 = train_label[sample_num:m]\n",
        "train_label2 = np.reshape(train_label2, (m-sample_num, 4))\n",
        "assert(train_label2.shape == (m-sample_num, 4))\n",
        "\n",
        "train_label1\n"
      ],
      "execution_count":71,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Initialization\n",
        "layer_dims: python array (list) containing the dimensions of each layer in our network\n",
        "\n",
        "parameters: python dictionary containing our parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "\n",
        "                    Wl: weight matrix \n",
        "                    bl: bias vector\n",
        "\n",
        "\n",
        "procedure:\n",
        "\n",
        "we assign a random number for every weight and a 0 for all biases \n",
        "\n",
        "**Dimensions**:\n",
        "\n",
        "            Wl: (layer_dims[l], layer_dims[l-1])\n",
        "            bl: (layer_dims[l], 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def initialize_parameters_deep(layer_dims):\n",
        "   \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        \n",
        "\n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count":3,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "p = initialize_parameters_deep([102,150,60,4])\n",
        "p['W1']"
      ],
      "execution_count":3,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Network Shape\n",
        "\n",
        "Our network will have 4 layers including output layer.\n",
        "\n",
        "In the input layer, we have 102 items followed by 2 hidden layers with 150 and 60 items.\n",
        "\n",
        "Also, in the output layer, we have for items which are: apple, lemon, mango and raspberry, respectively."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# parameters = initialize_parameters_deep([102,150,60,4])\n",
        "# parameters['W3'].shape\n",
        "# print(\"W1 = \" + str(parameters[\"W1\"]))\n",
        "# print(\"b1 = \" + str(parameters[\"b1\"]))\n",
        "# print(\"W2 = \" + str(parameters[\"W2\"]))\n",
        "# print(\"b2 = \" + str(parameters[\"b2\"]))\n",
        "# print(\"W3 = \" + str(parameters[\"W3\"]))\n",
        "# print(\"b3 = \" + str(parameters[\"b3\"]))"
      ],
      "execution_count":70,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function\n",
        "\n",
        "**Sigmoid:**\n",
        "    \n",
        "    Arguments:\n",
        "    Z:       numpy array of any shape\n",
        "    \n",
        "    Returns:\n",
        "    A:       output of sigmoid(z), same shape as Z\n",
        "    cache:   returns Z as well, useful during backpropagation\n",
        "\n",
        "**Relu:**\n",
        "\n",
        "    Arguments:\n",
        "    Z:       Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A:       Post-activation parameter, of the same shape as Z\n",
        "    cache:   storing \"A\" for computing the backward pass efficiently\n",
        "\n",
        "**Dimentions**:\n",
        "                \n",
        "                Z(l): (layer_dims(l), sample_num)\n",
        "                A(l): (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid(Z):\n",
        "    \n",
        "    A = 1\/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "\n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count":4,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu(Z):\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count":5,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation\n",
        "\n",
        "Implement the **linear part** of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A:      activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:      weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:      bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    Z:      the input of the activation function, also called pre-activation parameter.\n",
        "    cache:  storing \"A\", \"W\" and \"b\" for computing the backward pass efficiently.\n",
        "\n",
        "    **Dimensions**:\n",
        "\n",
        "                A:  (layer_dims(l-1), sample_num)\n",
        "                W:  (layer_dims(l), layer_dims(l-1))\n",
        "                b:  (layer_dims(l), 1)\n",
        "                Z:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_forward(A, W, b):\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count":6,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activate the linear part of Forward-Propagation\n",
        "\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION part\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "    A_prev:     activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W:          weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b:          bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "\n",
        "    A:          the output of the activation function, also called the post-activation value \n",
        "    cache:      storing \"linear_cache\" and \"activation_cache\" for computing the backward pass efficiently"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b) \n",
        "        A, activation_cache = sigmoid(Z) \n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "    \n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z) \n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    \n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count":7,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Forward-Propagation for all layers\n",
        "    \n",
        "**Arguments:**\n",
        "\n",
        "    X:           data, numpy array of shape (input size, number of examples)\n",
        "    parameters:  output of initialize_parameters_deep()\n",
        "    \n",
        "**Returns:**\n",
        "\n",
        "    A:          last post-activation value\n",
        "    caches:      list of caches containing:\n",
        "                    every cache of activating linear part of forward-propagation (there are L-1 of them, indexed from 0 to L-1)\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:  (layer_dims(l-1), sample_num)\n",
        "            A:  (layer_dims(l), sample_num)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L+1):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"sigmoid\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    assert(A.shape == (4,X.shape[1])) \n",
        "    # --------------------------------------\n",
        "    return A, caches"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "parameters = initialize_parameters_deep([102,150,60,4])\n",
        "A, caches = L_model_forward(train_value1,parameters)\n",
        "# caches = np.array(caches, dtype=object)\n",
        "# caches[0][0][0]"
      ],
      "execution_count":63,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Cost function\n",
        "\n",
        "\n",
        "We can use 2 ways to calculate the cost of network:\n",
        "\n",
        "                                                SSE\n",
        "                                                CE"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**SSE**"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_SSE(value, lable):\n",
        "    sse = 0\n",
        "    m = label[0].shape\n",
        "    for j in range(4):\n",
        "        sse += (label[j] - value[j])^2\n",
        "    \n",
        "    return sse"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "**Cross**-**Entropy**\n",
        "\n",
        "Arguments:\n",
        "\n",
        "        AL:     probability vector corresponding to our label predictions, shape (1, number of examples)\n",
        "        Y:      true \"label\" vector\n",
        "\n",
        "Returns:\n",
        "\n",
        "        cost:   cross-entropy cost\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "        cost:  (1)\n",
        "        Y:     (layer_dims(L), sample_num)  -> L is number of layers"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def compute_cost(AL, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (-1\/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
        "    cost = cost.sum()\n",
        "    cost = np.squeeze(cost)      # this turns [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count":9,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Back-Propagation\n",
        "\n",
        "We need to calculate Gradient of the cost respect to the parameters. then times it to a learning factor and finally upgrade the parameters.\n",
        "\n",
        "We will use 2 ways:\n",
        "                    one way is that we use some loops to iterate all over the network\n",
        "                    another way is that we use matrix operations "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Using Loops"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def calculate_gradient(value, lable):\n",
        "    for i in range(value.size):\n",
        "        A, caches = L_model_forward(value[i], parameters)\n",
        "        sse = compute_SSE(A.T, lable[i])"
      ],
      "execution_count":36,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Vectorization\n",
        "\n",
        "Using matrix operations"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Activation function Gradient\n",
        "\n",
        "Implement the backward propagation for a single **SIGMOID** unit.\n",
        "\n",
        "    Arguments:\n",
        "\n",
        "        dA:      post-activation gradient, of any shape\n",
        "        cache:   Z where we store for computing backward propagation efficiently\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        dZ:      Gradient of the cost with respect to Z\n",
        "\n",
        "    Dimensions:\n",
        "\n",
        "                dA:     (layer_dims(l), sample_num)\n",
        "                Z:      (layer_dims(l-1), sample_num)\n",
        "                dZ:     (layer_dims(l-1), sample_num)\n",
        "\n",
        "                here A in layer_dims(l) and Z in layer_dims(l-1) have equal dimentions."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def sigmoid_backward(dA, cache):\n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    s = 1\/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":10,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA -- post-activation gradient, of any shape\n",
        "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ -- Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, you should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count":8,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Gradient of parameters for a singl layer\n",
        "\n",
        "Here **cache** is \"linear_cache\" containing (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "   \n",
        "\n",
        "Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "Dimensions:\n",
        "\n",
        "            A_prev:      (layer_dims(l-1), sample_num)\n",
        "            W:           (layer_dims(l), layer_dims(l-1))\n",
        "            b:           (layer_dims(l), 1)\n",
        "            dz:          (layer_dims(l), sample_num)\n",
        "            dA_prev:     (layer_dims(l-1), sample_num)\n",
        "            dW:          (layer_dims(l), layer_dims(l-1))\n",
        "            db:          (layer_dims(l), 1)"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_backward(dZ, cache):\n",
        "   \n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1\/m) * np.dot(dZ, A_prev.T)\n",
        "    db = (1\/m) * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":11,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "    \n",
        "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count":12,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Obtain Gradient\n",
        "\n",
        "Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... "
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_model_backward(AL, Y, caches):\n",
        "   \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "\n",
        "    current_cache = caches[L-1] # Last Layer\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
        "#    فک کنم اینجا باید مستقیم نود لایه اخر رو بدیم یعنی نیازی به سیگموید نداره\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"sigmoid\")\n",
        "\n",
        "# ----------------------------------------\n",
        "    return grads"
      ],
      "execution_count":13,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Update Parameters\n",
        "\n",
        "Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ..."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    L = len(parameters) \/\/ 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "    return parameters"
      ],
      "execution_count":14,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Get Accuracy"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_accuracy(predictions, Y):\n",
        "    # print(\"predictions: \", predictions)\n",
        "    return np.sum(predictions == Y) \/ Y.shape[1]"
      ],
      "execution_count":95,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Prediction"
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def get_predictions(A):\n",
        "    return np.argmax(A, 0)"
      ],
      "execution_count":16,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"markdown",
      "source":[
        "# Implement a L-layer neural network\n",
        "\n",
        " Arguments:\n",
        "\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    parameters -- parameters learnt by the model. They can then be used to predict."
      ],
      "attachments":{
        
      },
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "def L_layer_model(data, label, layers_dims, learning_rate, num_iterations, print_cost):\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "    \n",
        "        \n",
        "        # shuffle = np.random.permutation(data.shape[1])\n",
        "        # data = data.T[shuffle]\n",
        "        # label = label[shuffle]\n",
        "\n",
        "        # Forward propagation:\n",
        "        AL, caches = L_model_forward(data, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, label)\n",
        "    \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, label, caches)\n",
        " \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # print(\"parameters: \", parameters[\"W2\"][10][2])\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 10 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "            # print(\"AL: \", AL)\n",
        "            predictions = get_predictions(AL)\n",
        "            # print(\"predictions: \", predictions)\n",
        "            print(\"Accuracy: \", get_accuracy(predictions, label))\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count":131,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "Y = np.array([[0.0, 0.0, 1.0, 0.0],\n",
        "            [1.0, 0.0, 0.0, 0.0],\n",
        "            [0.0, 1.0, 0.0, 0.0],\n",
        "            [0.0, 0.0, 0.0, 1.0]])\n",
        "predict = np.array([2, 0, 1, 3])\n",
        "s = np.sum(predict == Y) \/ Y.shape[1]\n",
        "s"
      ],
      "execution_count":42,
      "outputs":[
        {
          "data":{
            "text\/plain":[
              "1.0"
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "layers_dims = [102, 150, 60, 4]\n",
        "parameters = L_layer_model(train_value1, train_label1.T, layers_dims, 0.5, 40, True)"
      ],
      "execution_count":134,
      "outputs":[
        {
          "name":"stdout",
          "text":[
            "Cost after iteration 0: 11.175704\n",
            "Accuracy:  0.0\n",
            "Cost after iteration 10: 9.015538\n",
            "Accuracy:  1.0\n",
            "Cost after iteration 20: 9.015491\n",
            "Accuracy:  1.0\n",
            "Cost after iteration 30: 9.015491\n",
            "Accuracy:  1.0\n"
          ],
          "output_type":"stream"
        },
        {
          "data":{
            "image\/png":[
              "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAkvUlEQVR4nO3deZhU9Z3v8fe3F2j2tdmXZunGfe3gCkEFokmMjsbEmBg1jqLRuODceXLnuTeTyUzmmkVQo9FoVNREo8Yk4g4IBFxQGwYURJpF9q3Zl2br5nv\/qIMp2uqm6O6qX1X35\/U89fSpU6eqPqcK6lO\/OlXnmLsjIiJSU07oACIikplUECIikpAKQkREElJBiIhIQioIERFJSAUhIiIJqSBEImY2zMwWhc4hkilUEJIRzGy5mY0MmcHdZ7r7kJAZDjGzEWa2Ok33dYGZfWpmlWY2zcz617HscjPbY2a7otOkdGSUMFQQ0myYWW7oDAAWkxH\/98ysK\/AX4P8CnYEy4LkjXO1id28bnUanOqOEkxH\/SEVqY2Y5ZvZjM1tqZpvN7Hkz6xx3+Qtmtt7MtpvZDDM7Pu6yCWb2kJm9Zma7gfOid8D\/YmYfRdd5zswKouUPe9de17LR5f9qZuvMbK2Z\/bOZuZkNrmU9ppvZz83sHaASGGhm15nZQjPbaWbLzGxMtGwb4HWgV9w79V5Heizq6TJggbu\/4O57gZ8CJ5vZMQ28XWkCVBCS6X4EXAp8GegFbAUejLv8daAY6AbMAf5Y4\/pXAT8H2gFvR\/O+BVwIDABOAq6t4\/4TLmtmFwJjgZHAYGBEEutyNXBjlGUFsBH4OtAeuA4Yb2anuftu4CJgbdw79bVJPBafM7N+ZratjtNV0aLHA\/MOXS+676XR\/Nr80cwqzGySmZ2cxHpLlsoLHUDkCG4CbnX31QBm9lNgpZld7e5V7v74oQWjy7aaWQd33x7Nfsnd34mm95oZwP3RCy5m9jJwSh33X9uy3wKecPcFcff93SOsy4RDy0dejZv+e\/R5\/jBiRZdInY9F\/ILuvhLoeIQ8AG2BihrzthMrsUS+G+Uz4HbgTTM7xt23JXFfkmU0gpBM1x\/466F3vsBCoBrobma5ZnZ39JHLDmB5dJ2ucddfleA218dNVxJ7kaxNbcv2qnHbie6npsOWMbOLzGyWmW2J1u2rHJ69plofiyTuuza7iI1g4rUHdiZa2N3fcfc97l7p7v8P2Eas1KQJUkFIplsFXOTuHeNOBe6+htjHR5cQ+5inA1AUXcfirp+q3RWvA\/rEne+bxHU+z2JmLYEXgV8D3d29I\/Aa\/8ieKHddj8Vhoo+YdtVxOjTaWQCcHHe9NsCgaH4ynMMfb2lCVBCSSfLNrCDulAc8DPz80FcvzazQzC6Jlm8H7AM2A62B\/05j1ueB68zsWDNrTexbQEejBdCS2Mc7VWZ2ERD\/jaANQBcz6xA3r67H4jDuvjJu+0Wi06FtNX8FTjCzy6MN8D8BPnL3T2veZlQ655hZi+j5+V\/ERjzv1FxWmgYVhGSS14A9caefAvcBE4FJZrYTmAWcES3\/FLGNvWuAT6LL0sLdXwfuB6YBS+Lue1+S198J3EasaLYSGw1NjLv8U+BZYFn0kVIv6n4s6rseFcDlxDbkb41u78pDl5vZw2b2cHS2HfBQtNwaYhvvL3L3zQ3JIJnLdMAgkYYzs2OB+UDLmhuMRbKVRhAi9WRm\/2RmLc2sE\/AL4GWVgzQlKgiR+htD7LcMS4l9m+jmsHFEGpc+YhIRkYQ0ghARkYSazC+pu3bt6kVFRaFjiIhkldmzZ29y98JElzWZgigqKqKsrCx0DBGRrGJmK2q7TB8xiYhIQioIERFJSAUhIiIJqSBERCQhFYSIiCSkghARkYRUECIiklCzL4gdew\/w6zcX8dmm3aGjiIhklGZfEPsOHOSxtz\/jvinloaOIiGSUZl8Qhe1acs3ZRbw0by2LNyQ8DK+ISLPU7AsCYMzwgbRpkcd4jSJERD6nggA6tWnBD84dwGsfr2fB2u2h44iIZAQVROT6cwfQviCP8ZM1ihARARXE5zq0ymfMlwcxZeFG5q7aFjqOiEhwKog4155dROc2LRinUYSIiAoiXpuWedz05YHMKK\/gw+VbQscREQlKBVHD1WcWUdiuJb9+cxE6XreINGcqiBpatcjllhGDeP+zLby7dHPoOCIiwaggEvjOGf3o1aGAeyZpFCEizZcKIoGWebncen4xc1ZuY3p5Reg4IiJBqCBqcUVpH\/p2bsW4SeUaRYhIs5SygjCzx81so5nNj5t3hZktMLODZlZax3UvNLNFZrbEzH6cqox1yc\/N4fYLSvh4zXYmfbIhRAQRkaBSOYKYAFxYY9584DJgRm1XMrNc4EHgIuA44DtmdlyKMtbp0lN6MbBrG8ZNKufgQY0iRKR5SVlBuPsMYEuNeQvdfdERrjoUWOLuy9x9P\/An4JIUxaxTXm4Od4wqYdGGnbz68boQEUREgsnEbRC9gVVx51dH877AzG40szIzK6uoSM3G5K+f2JMh3dsxfko5VdUHU3IfIiKZKBMLImnu\/oi7l7p7aWFhYUruIyfHuHNUMcsqdvPS3LUpuQ8RkUyUiQWxBugbd75PNC+Yrxzfg+N7tee+txZzQKMIEWkmMrEgPgSKzWyAmbUArgQmhgxkZtw1uoSVWyr58+zVIaOIiKRNKr\/m+izwHjDEzFab2fVm9k9mtho4C3jVzN6Mlu1lZq8BuHsVcCvwJrAQeN7dF6QqZ7LOG9KNU\/t15DdvLWZfVXXoOCIiKWdN5UdgpaWlXlZWltL7eHvxJr732Pv87JLj+f5ZRSm9LxGRdDCz2e6e8HdpmfgRU8Y6Z3AXhg7ozANTl7D3gEYRItK0qSCOgplx16gSNu7cxx9mrQgdR0QkpVQQR+mMgV0YVtyV305fyu59VaHjiIikjAqiHsaOKmHL7v1MeHd56CgiIimjgqiHU\/t14oJjuvHIjGXs2HsgdBwRkZRQQdTTnaNK2L7nAI\/N\/Cx0FBGRlFBB1NMJvTtw0Qk9ePztz9i6e3\/oOCIijU4F0QB3jiph1\/4qHpm5LHQUEZFGp4JogJLu7bj4pF5MeGc5m3btCx1HRKRRqSAa6I6Rxeyrqubh6UtDRxERaVQqiAYaWNiWy07rw9OzVrBhx97QcUREGo0KohHcfkEx1QedB6ctCR1FRKTRqCAaQd\/OrfnWl\/ry7AcrWb21MnQcEZFGoYJoJLeeNxjDeGCqRhEi0jSoIBpJr46tuOqMfrwwezXLN+0OHUdEpMFUEI3oh+cNIj\/XuP+txaGjiIg0mAqiEXVrV8A1ZxXxt7lrWLJxZ+g4IiINooJoZGO+PIhW+bmMn6JRhIhkNxVEI+vcpgXXnTOAVz9ax8J1O0LHERGpNxVECtwwbCDtCvIYP7k8dBQRkXpTQaRAh9b53DBsIJM+2cBHq7eFjiMiUi8qiBS57pwiOrXOZ5xGESKSpVQQKdKuIJ8xXx7E9EUVzF6xJXQcEZGjpoJIoe+f1Z+ubVtwzySNIkQk+6ggUqh1izx+OGIw7y7dzLtLN4WOIyJyVFQQKXbVGf3o0b6AcZPKcffQcUREkqaCSLGC\/FxuPX8wZSu2MmOxRhEikj1UEGnwrdK+9OnUinGTFmkUISJZQwWRBi3ycrjt\/GLmrd7OlIUbQ8cREUmKCiJNLjutN0VdWjNucjkHD2oUISKZTwWRJnm5OdwxsoSF63bw+vz1oeOIiByRCiKNLj65F8Xd2jJ+SjnVGkWISIZTQaRRbo5x56gSlmzcxcvz1oaOIyJSJxVEml14fA+O7dmee6eUU1V9MHQcEZFaqSDSLCfHuGtUCcs3V\/KXOWtCxxERqZUKIoALju3GyX07ct9bi9lfpVGEiGQmFUQAZsbYUSWs2baH58pWhY4jIpJQygrCzB43s41mNj9uXmczm2xmi6O\/nWq5brWZzY1OE1OVMaThxV35UlEnHpi6mL0HqkPHERH5glSOICYAF9aY92PgLXcvBt6Kzieyx91PiU7fSGHGYGKjiCFs2LGPP76\/MnQcEZEvSFlBuPsMoOaRci4BnoymnwQuTdX9Z4OzBnXhnMFdeGj6Eir3V4WOIyJymHRvg+ju7uui6fVA91qWKzCzMjObZWaX1nZjZnZjtFxZRUVFY2dNi7GjhrBp136efHdF6CgiIocJtpHaY7s1re3nxP3dvRS4CrjXzAbVchuPuHupu5cWFhamKmpKnd6\/EyOGFPK7GUvZufdA6DgiIp9Ld0FsMLOeANHfhLs2dfc10d9lwHTg1HQFDOGuUUPYVnmAJ95ZHjqKiMjn0l0QE4FroulrgJdqLmBmncysZTTdFTgH+CRtCQM4sU8HRh\/XnUdnLmN7pUYRIpIZUvk112eB94AhZrbazK4H7gZGmdliYGR0HjMrNbPfR1c9Figzs3nANOBud2\/SBQEwdnQJu\/ZV8ejMZaGjiIgAkJeqG3b379Ry0QUJli0D\/jmafhc4MVW5MtUxPdrztRN78vg7n3HdOUV0adsydCQRaeb0S+oMcsfIEvYeqOZ3MzSKEJHwVBAZZHC3tlx6am+eem85G3fsDR1HRJo5FUSGuf2CYg5UO7+dvjR0FBFp5lQQGaZ\/lzZ8q7QPz7y\/krXb9oSOIyLNmAoiA916fjEAv5m6JHASEWnOVBAZqHfHVlw5tC8vlK1i5ebK0HFEpJlSQWSoW84bTG6Ocf\/UxaGjiEgzpYLIUN3bF3D1mf35y5zVLK3YFTqOiDRDKogMdtOIQRTk53LfFI0iRCT9VBAZrGvbllx7dhEvf7SWRet3ho4jIs2MCiLD3Th8IG1b5DF+cnnoKCLSzKggMlzH1i24ftgA3liwnvlrtoeOIyLNiAoiC\/zg3AF0aJXPOI0iRCSNVBBZoH1BPmO+PJCpn25kzsqtoeOISDOhgsgS15xVRJc2LRg3SaMIEUkPFUSWaNMyj5tHDOLtJZt4f9nm0HFEpBlQQWSR753Zn27tWnLPpHLcPXQcEWniVBBZpCA\/l1vPH8wHy7fw9pJNoeOISBOngsgy3\/5SX3p1KNAoQkRSTgWRZVrm5XLbBcXMXbWNaYs2ho4jIk2YCiILXX56H\/p1bq1RhIiklAoiC+Xn5nDHyGIWrN3BmwvWh44jIk2UCiJLXXJKbwYVtmHc5HKqD2oUISKNL6mCMLMrkpkn6ZObY9wxsoTyDbt45aO1oeOISBOU7Ajifyc5T9Loayf25Jge7bhvymKqqg+GjiMiTUxeXRea2UXAV4HeZnZ\/3EXtgapUBpMjy8kx7hxVwpinZ\/PX\/1nDFaV9Q0cSkSbkSCOItUAZsBeYHXeaCHwltdEkGaOP686JvTtw\/9TF7K\/SKEJEGk+dBeHu89z9SWCwuz8ZTU8Elri7diuaAcyMsaNLWLVlDy\/MXhU6jog0Iclug5hsZu3NrDMwB3jUzManMJcchRElhZzWryMPTF3C3gPVoeOISBORbEF0cPcdwGXAU+5+BnBB6mLJ0TAz\/mX0ENZt38ufPlgZOo6INBHJFkSemfUEvgW8ksI8Uk9nD+7KmQM788C0pezZr1GEiDRcsgXxM+BNYKm7f2hmA4HFqYsl9XHX6CFs2rWPp2ctDx1FRJqApArC3V9w95Pc\/ebo\/DJ3vzy10eRofamoM8NLCnlo+lJ27dO3kEWkYZL9JXUfM\/urmW2MTi+aWZ9Uh5OjN3ZUCVsrDzDhnc9CRxGRLJfsR0xPEPt6a6\/o9HI0TzLMKX07MvLY7jwyYxnb9xwIHUdEsliyBVHo7k+4e1V0mgAUpjCXNMDYUSXs2FvFYzOXhY4iIlks2YLYbGbfM7Pc6PQ9YHMqg0n9HderPV87sSePv7OcLbv3h44jIlkq2YL4AbGvuK4H1gHfBK6t6wpm9ni0vWJ+3LzOZjbZzBZHfzvVct1romUWm9k1SWaUOHeMLGb3\/ip+N2Np6CgikqWO5muu17h7obt3I1YY\/3GE60wALqwx78fAW+5eDLwVnT9M9GvtfwfOAIYC\/15bkUjtiru345KTe\/HUuyuo2LkvdBwRyULJFsRJ8ftecvctwKl1XcHdZwBbasy+BHgymn4SuDTBVb8CTHb3LdF9TuaLRSNJuH1kCfurD\/LQdI0iROToJVsQOfHv4qN3+XXuKrwW3d19XTS9HuieYJneQPxe51ZH877AzG40szIzK6uoqKhHnKZtQNc2XH5ab\/7w\/grWbd8TOo6IZJlkC+Ie4D0z+08z+0\/gXeCXDbljd3egQcfKdPdH3L3U3UsLC\/WlqkR+dH4x7s6D05aEjiIiWSbZX1I\/RWxHfRui02Xu\/nQ97m9DtE8nor8bEyyzBog\/8k2faJ7UQ9\/Orfn2l\/ry3IerWLWlMnQcEckiyY4gcPdP3P2B6PRJPe9vInDoW0nXAC8lWOZNYLSZdYo+1hodzZN6uvW8YsyM30zV7rNEJHlJF8TRMrNngfeAIWa22syuB+4GRpnZYmBkdB4zKzWz38PnG8D\/E\/gwOv0smif11KNDAd87oz8vzlnDZ5t2h44jIlnCYpsCsl9paamXlZWFjpGxKnbuY\/gvp\/GV47tz75V1fgFNRJoRM5vt7qWJLkvZCEIyS2G7llxzdhEvzVvL4g07Q8cRkSyggmhGxgwfSJsWeYyfUh46iohkARVEM9KpTQt+cE4Rr328ngVrt4eOIyIZTgXRzFw\/bCDtC\/IYP1nfaBKRuqkgmpkOrfK5cfhApizcwNxV20LHEZEMpoJohq49ZwCdWuczbrK2RYhI7VQQzVDblnncPGIQM8or+HC5fmIiIompIJqpq88sorBdS+6ZtCh0FBHJUCqIZqpVi1xuGTGIWcu28O6STaHjiEgGUkE0Y1cO7UfPDgX8etIimsov6kWk8aggmrGC\/FxuPX8wc1ZuY3q5jqchIodTQTRzV5zel76dWzFuUrlGESJyGBVEM9ciL4fbzi\/m4zXbmfTJhtBxRCSDqCCEfzq1NwO7tmH85HIOHtQoQkRiVBBCXm4Ot48s5tP1O3n143VHvoKINAsqCAHg4pN6UdK9LfdOKadaowgRQQUhkZwcY+yoEpZW7OaluToEuIioICTOV47vwfG92nPvlMUcqD4YOo6IBKaCkM+ZGXeNLmHllkpenL06dBwRCUwFIYc5b0g3TunbkfvfWsy+qurQcUQkIBWEHObQKGLt9r089+Gq0HFEJCAVhHzBuYO7MnRAZx6YuoS9BzSKEGmuVBDyBWbGXaNK2LhzH3+YtSJ0HBEJRAUhCZ0xsAvDirvy0PSl7N5XFTqOiASggpBajR1Vwubd+5nw7vLQUUQkABWE1OrUfp04\/5huPDJjGTv2HggdR0TSTAUhdRo7qoTtew7w+NufhY4iImmmgpA6ndC7Axce34PHZn7Gtsr9oeOISBqpIOSI7hxVwq79VTwyY1noKCKSRioIOaIhPdpx8Um9eOKd5WzatS90HBFJExWEJOX2kcXsq6rm4elLQ0cRkTRRQUhSBhW25bLT+vD0rBVs2LE3dBwRSQMVhCTt9guKqT7oPDhtSegoIpIGKghJWt\/OrbmitC\/PfrCS1VsrQ8cRkRRTQchR+dH5gzGMB6ZqFCHS1Kkg5Kj06tiKq87oxwuzV7Ni8+7QcUQkhYIUhJndbmbzzWyBmd2R4PIRZrbdzOZGp58EiCm1+OGIQeTnGve9tTh0FBFJobQXhJmdANwADAVOBr5uZoMTLDrT3U+JTj9La0ipU7f2BXz\/rCL+9j9rWLJxV+g4IpIiIUYQxwLvu3ulu1cBfwcuC5BDGmDM8IEU5Ody75Ty0FFEJEVCFMR8YJiZdTGz1sBXgb4JljvLzOaZ2etmdnx6I8qRdGnbkh+cM4BXPlrHwnU7QscRkRRIe0G4+0LgF8Ak4A1gLlDzuJZzgP7ufjLwG+BviW7LzG40szIzK6uoqEhZZknshmEDaVeQx\/jJGkWINEVBNlK7+2Pufrq7Dwe2AuU1Lt\/h7rui6deAfDPrmuB2HnH3UncvLSwsTEt2+YcOrfO5YdhAJn2ygY9Xbw8dR0QaWahvMXWL\/vYjtv3hmRqX9zAzi6aHEsu5Od055ciuO6eIjq3zuWfyotBRRKSRhfodxItm9gnwMnCLu28zs5vM7Kbo8m8C881sHnA\/cKW7e6CsUod2BfmMGT6I6YsqmL1iS+g4ItKIrKm87paWlnpZWVnoGM1S5f4qhv9yGiXd2\/HMDWeGjiMiR8HMZrt7aaLL9EtqabDWLfK4ecRg3l26mfeW6pNAkaZCBSGN4rtn9KNH+wLGTV5EUxmVijR3KghpFAX5udxy\/mA+XL6VGYs3hY4jIo1ABSGN5tulfendsRXjJmkUIdIUqCCk0bTIy+H2C4qZt3o7UxZuDB1HRBpIBSGN6rLTelPUpTXjJpdz8KBGESLZTAUhjSovN4c7RpawcN0O3liwPnQcEWkAFYQ0uotP7kVxt7aMm1xOtUYRIllLBSGNLjfHuGNkCUs27uLleWtDxxGRelJBSEpcdEIPju3ZnnunlFNVfTB0HBGpBxWEpEROjjF2VAnLN1fylzlrQscRkXpQQUjKjDy2Gyf36cB9by1mf5VGESLZRgUhKWNmjB09hDXb9vBc2arQcUTkKKkgJKWGF3eltH8nHpy6hL0Hah44UEQymQpCUsrMuGv0ENbv2Msz768MHUdEjoIKQlLurEFdOHtQF347fQmV+6tCxxGRJKkgJC3uGl3Cpl37eeq9FaGjiEiSVBCSFqf378yIIYU8\/Pel7Nx7IHQcEUmCCkLSZuyoErZVHuCJd5aHjiIiSVBBSNqc1Kcjo4\/rzqMzl7G9UqMIkUyngpC0unNUCTv3VvHozGWho4jIEaggJK2O7dmer53Ukyfe+YzNu\/aFjiMidVBBSNrdObKYPQeq+d0MjSJEMpkKQtJucLd2XHpKb556bzkbd+4NHUdEaqGCkCBuH1nMgWrnt9OWho4iIrVQQUgQ\/bu04YrT+\/DM+ytZu21P6DgikoAKQoK59fzBOM4D05aEjiIiCaggJJg+nVrznaH9eP7DVazcXBk6jojUoIKQoG45bzC5Ocb9UxeHjiIiNaggJKju7Qu4+sz+\/GXOapZV7AodR0TiqCAkuJtGDKIgP5d7p2gUIZJJVBASXNe2Lbnm7CJe\/mgti9bvDB1HRCIqCMkIY4YPpG2LPMZPLg8dRUQiKgjJCB1bt+AH5w7gjQXrmb9me+g4IoIKQjLI9cMG0KFVvkYRIhlCBSEZo31BPjcOH8hbn25kzsqtoeOINHsqCMko155dRJc2LTSKEMkAQQrCzG43s\/lmtsDM7khwuZnZ\/Wa2xMw+MrPTAsSUANq0zOPmEYOYuXgT7y\/bHDqOSLOWl+47NLMTgBuAocB+4A0ze8Xd43fIcxFQHJ3OAB6K\/koz8L0z+\/PIjGWM+cNsurZtGTqO1GChA8gXHNOzPb\/5zqmNfrtpLwjgWOB9d68EMLO\/A5cBv4xb5hLgKXd3YJaZdTSznu6+Lv1xJd0K8nP5xeUn8ec5q0NHaRweOkDj8aa0Mk1I306tUnK7IQpiPvBzM+sC7AG+CpTVWKY3sCru\/Opo3mEFYWY3AjcC9OvXL1V5JYDzjunGecd0Cx1DpFlL+zYId18I\/AKYBLwBzAWq63lbj7h7qbuXFhYWNl5IEREJs5Ha3R9z99PdfTiwFaj5lZU1QN+4832ieSIikiahvsXULfrbj9j2h2dqLDIR+H70baYzge3a\/iAikl4htkEAvBhtgzgA3OLu28zsJgB3fxh4jdi2iSVAJXBdoJwiIs1WkIJw92EJ5j0cN+3ALWkNJSIih9EvqUVEJCEVhIiIJKSCEBGRhCz2cX\/2M7MKYEUDbqIrsKmR4oTUVNYDtC6ZqqmsS1NZD2jYuvR394Q\/JGsyBdFQZlbm7qWhczRUU1kP0LpkqqayLk1lPSB166KPmEREJCEVhIiIJKSC+IdHQgdoJE1lPUDrkqmayro0lfWAFK2LtkGIiEhCGkGIiEhCKggREUmoWRWEmV1oZouiY13\/OMHlLc3suejy982sKEDMpCSxLteaWYWZzY1O\/xwi55GY2eNmttHM5tdyedYcnzyJdRlhZtvjnpOfpDtjMsysr5lNM7NPouPG355gmax4XpJcl2x5XgrM7AMzmxety38kWKZxX8PcvVmcgFxgKTAQaAHMA46rscwPgYej6SuB50LnbsC6XAs8EDprEusyHDgNmF\/L5V8FXid2KOQziR2uNnjueq7LCOCV0DmTWI+ewGnRdDtix2up+e8rK56XJNclW54XA9pG0\/nA+8CZNZZp1New5jSCGAoscfdl7r4f+BOxY1\/HuwR4Mpr+M3CBmWXiMdqTWZes4O4zgC11LPL58cndfRbQ0cx6pifd0UliXbKCu69z9znR9E5gIbFD\/sbLiuclyXXJCtFjvSs6mx+dan7LqFFfw5pTQdR2nOuEy7h7FbAd6JKWdEcnmXUBuDwa\/v\/ZzPomuDwbJLuu2eKs6COC183s+NBhjiT6iOJUYu9W42Xd81LHukCWPC9mlmtmc4GNwGR3r\/V5aYzXsOZUEM3Ny0CRu58ETOYf7yoknDnE9ntzMvAb4G9h49TNzNoCLwJ3uPuO0Hka4gjrkjXPi7tXu\/spxA7DPNTMTkjl\/TWngkjmONefL2NmeUAHYHNa0h2dI66Lu292933R2d8Dp6cpW2NrMscnd\/cdhz4icPfXgHwz6xo4VkJmlk\/sBfWP7v6XBItkzfNypHXJpuflEHffBkwDLqxxUaO+hjWngvgQKDazAWbWgtgGnIk1lpkIXBNNfxOY6tHWngxzxHWp8XnwN4h99pqNmszxyc2sx6HPg81sKLH\/fxn3BiTK+Biw0N3H1bJYVjwvyaxLFj0vhWbWMZpuBYwCPq2xWKO+hoU6JnXauXuVmd0KvEnsW0CPu\/sCM\/sZUObuE4n9Q3razJYQ29h4ZbjEtUtyXW4zs28AVcTW5dpggetgZs8S+xZJVzNbDfw7sY1veJYdnzyJdfkmcLOZVQF7gCsz9A3IOcDVwMfR590A\/wb0g6x7XpJZl2x5XnoCT5pZLrESe97dX0nla5h2tSEiIgk1p4+YRETkKKggREQkIRWEiIgkpIIQEZGEVBAiIpKQCkLSyszejf4WmdlVjXzb\/5bovlLFzC5N1Z4\/zWzXkZeq1+2OMLNXGngby+v6IZmZ\/cnMihtyH5IZVBCSVu5+djRZBBxVQUS\/DK3LYQURd1+p8q\/Abxt6I0msV8o1coaHiD02kuVUEJJWce+M7waGRfvfvzPaCdmvzOzDaAeDY6LlR5jZTDObCHwSzfubmc2O9ol\/YzTvbqBVdHt\/jL+v6Ne+vzKz+Wb2sZl9O+62p0c7M\/zUzP4Y94vauy12DIGPzOzXCdajBNjn7pui8xPM7GEzKzOzcjP7ejQ\/6fVKcB8\/t9gO5GaZWfe4+\/lmzcfzCOtyYTRvDnBZ3HV\/amZPm9k7xH5cVWhmL0ZZPzSzc6LlupjZpOjx\/j2x3U5jZm3M7NUo4\/xDjyswExiZCcUnDRRq3+Y6Nc8TsCv6O4K4ffADNwL\/J5puCZQBA6LldgMD4pbtHP1tBcwHusTfdoL7upzYDgtzge7ASmK\/Sh1BbG+XfYi9WXoPOJfY3i8X8Y8fknZMsB7XAffEnZ8AvBHdTjGxvZsWHM161bh9By6Opn8ZdxsTgG\/W8ngmWpcCYnv3LCb2wv78occd+CkwG2gVnX8GODea7kds9xQA9wM\/iaa\/FmXrGj2uj8Zl6RA3PRk4PfS\/N50adtIIQjLFaGL79plLbHfMXYi9qAF84O6fxS17m5nNA2YR2zHZkT7vPhd41mN7wtwA\/B34Utxtr3b3g8BcYh99bQf2Ao+Z2WXEdiVRU0+gosa85939oLsvBpYBxxzlesXbDxzaVjA7ynUkidblGOAzd1\/ssVfuP9S4zkR33xNNjwQeiLJOBNpbbC+oww9dz91fBbZGy38MjDKzX5jZMHffHne7G4FeSWSWDKYhoGQKA37k7m8eNtNsBLF32vHnRwJnuXulmU0n9i65vvbFTVcDeR7b19VQ4AJi++m5FTi\/xvX2ENtTZrya+61xklyvBA5EL+if54qmq4g+GjazHGJHFKx1Xeq4\/UPiM+QQO0LZ3hpZE17R3cstdqjRrwL\/ZWZvufvPoosLiD1GksU0gpBQdhI7BOQhbxLbYVo+xD7jN7M2Ca7XAdgalcMxxA53eciBQ9evYSbw7Wh7QCGxd8Qf1BYsetfcwWO7fr4TODnBYguBwTXmXWFmOWY2iNjhYBcdxXolazn\/2HX7N4h2BliHT4GiKBPAd+pYdhLwo0NnzOyUaHIG0RcKzOwioFM03QuodPc\/AL8idrjVQ0qIffwnWUwjCAnlI6A6+qhoAnAfsY9E5kQbVyuASxNc7w3gJjNbSOwFeFbcZY8AH5nZHHf\/btz8vwJnETt2twP\/6u7ro4JJpB3wkpkVEBsBjE2wzAzgHjOzuHf6K4kVT3vgJnffG23UTWa9kvVolG0esceirlEIUYYbgVfNrJJYWbarZfHbgAfN7CNirw0zgJuA\/wCeNbMFwLvRegKcCPzKzA4CB4CbAaIN6nvcfX39V1MygfbmKlJPZnYf8LK7TzGzCcQ2\/v45cKzgzOxOYIe7PxY6izSMPmISqb\/\/BlqHDpGBtqFD3DYJGkGIiEhCGkGIiEhCKggREUlIBSEiIgmpIEREJCEVhIiIJPT\/AYFUP7U6j+uPAAAAAElFTkSuQmCC\n"
            ]
          },
          "metadata":{
            "image\/png":{
              
            }
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "# pred_train = predict(train_value1, train_lable1, parameters)"
      ],
      "execution_count":0,
      "outputs":[
        
      ],
      "metadata":{
        
      }
    },
    {
      "cell_type":"code",
      "source":[
        "a = np.arange(6).reshape(2,3) + 10\n",
        "np.argmax(a, 0)"
      ],
      "execution_count":26,
      "outputs":[
        {
          "data":{
            "text\/html":[
              
            ]
          },
          "metadata":{
            
          },
          "output_type":"display_data"
        }
      ],
      "metadata":{
        
      }
    }
  ],
  "metadata":{
    
  },
  "nbformat":4,
  "nbformat_minor":0
}